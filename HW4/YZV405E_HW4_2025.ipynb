{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "damD0CT_lGFs"
      },
      "source": [
        "# **YZV405E - Natural Language Processing / Homework 4**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-St3JA9nUQC"
      },
      "source": [
        "## **1. Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQz0fwr6lLe1",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "**Any modification to the structure of the notebook is strictly prohibited.\n",
        "\n",
        "In this notebook, you'll implement a recurrent neural network that performs sentiment analysis.\n",
        ">Using an RNN rather than a strictly feedforward network is more accurate since we can include information about the *sequence* of words.\n",
        "\n",
        "Here we'll use a dataset of food reviews, accompanied by sentiment labels: positive or negative.\n",
        "\n",
        "### **Network Architecture**\n",
        "\n",
        "The architecture for this network is shown below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=11tNLfadUaI8bUrSQm-LoiQye_YEq7-vG\" width=80%>\n",
        "\n",
        ">**First, we'll pass in words to an embedding layer.** We need an embedding layer because we have tens of thousands of words, so we'll need a more efficient representation for our input data than one-hot encoded vectors. One approach is having an embedding layer and let the network learn a different embedding table on its own. *In this case, the embedding layer is for dimensionality reduction, rather than for learning semantic representations.* Other method you will implement is utilizing a pre-trained language model (such as fastText) and giving word embeddings that you obtained from this model to your network.\n",
        "\n",
        ">**After input words are passed to an embedding layer, the new embeddings will be passed to LSTM cells.** The LSTM cells will add *recurrent* connections to the network and give us the ability to include information about the *sequence* of words in the movie review data.\n",
        "\n",
        ">**Finally, the LSTM outputs will go to a sigmoid output layer.** We're using a sigmoid function because positive and negative = 1 and 0, respectively, and a sigmoid will output predicted, sentiment values between 0-1.\n",
        "\n",
        "We don't care about the sigmoid outputs except for the **very last one**; we can ignore the rest. We'll calculate the loss by comparing the output at the last time step and the training label (pos or neg)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehaPAgmhnV9m"
      },
      "source": [
        "## **2. Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF8cuy4lnbJv"
      },
      "source": [
        "\n",
        "The dataset consist of text reviews and corresponding sentiment labels. Here's a breakdown:\n",
        "\n",
        "**Review:** This column contains text data representing customer reviews written in Turkish.\n",
        "\n",
        "**Sentiment:** This column contains numerical values representing the sentiment or emotion associated with each review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI2k6VBuoAzW"
      },
      "source": [
        "### **Load Data**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IQpEi0D-QiF"
      },
      "source": [
        "If you are using Google Colab, you may create a folder in your Google Drive domain and get the dataset from that folder. Please make sure you have at least 10GB of free space available in your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkaD2n3y-QiG",
        "outputId": "ba967a7a-0a1d-40f9-9b7d-8a8628dc946b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Comment out this cell if you are working on Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "HW_PATH = '/content/drive/MyDrive/YZV405E_HW4/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF70Jbnroh8U"
      },
      "source": [
        "If you are running this notebook in your local, simply get the current directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKaAQBPoocVU"
      },
      "outputs": [],
      "source": [
        "# Uncomment this cell if you are not working on Google Colab\n",
        "#import os\n",
        "#HW_PATH = os.getcwd()\n",
        "#print(HW_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2OhHJ9x0ozO7",
        "outputId": "53183b65-71e8-4e15-e56c-f2b1c0da282d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 13751,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12469,\n        \"samples\": [\n          \"Pancarl\\u0131 sos enfesss a\\u015fk\\u0131n bey i tebrik ediyorum ata\\u015fehir deki en ba\\u015far\\u0131l\\u0131 burgerci the hamburger.\",\n          \"\\u00c7okkk lezzetliydi ellerinize sa\\u011fl\\u0131k. Yaln\\u0131z bowldaki et \\u00e7ok azd\\u0131. Tavuklu bowldan10 lira fazla verip az\\u0131c\\u0131k et g\\u00f6rmek \\u00fcz\\u00fcc\\u00fc oldu biraz.\",\n          \"H\\u0131zl\\u0131 hizmett.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a168062b-cb24-44b5-b821-a482139e5f28\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Her zaman komşu fırından sipariş verdiğim için...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sosisli ürün isteyen adama peynirli bişey yol...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Iyi pişsin diye söylememe rağmen az pişmiş gel...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>kokmuş hamburger getirdiniz be ayıp ulan resm...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Allah affetsin çok kötüydü hiç bir şey mi iyi ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a168062b-cb24-44b5-b821-a482139e5f28')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a168062b-cb24-44b5-b821-a482139e5f28 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a168062b-cb24-44b5-b821-a482139e5f28');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-787c9006-1270-43e1-8625-e8fb33e6d2d3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-787c9006-1270-43e1-8625-e8fb33e6d2d3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-787c9006-1270-43e1-8625-e8fb33e6d2d3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  Her zaman komşu fırından sipariş verdiğim için...          0\n",
              "1   sosisli ürün isteyen adama peynirli bişey yol...          0\n",
              "2  Iyi pişsin diye söylememe rağmen az pişmiş gel...          0\n",
              "3   kokmuş hamburger getirdiniz be ayıp ulan resm...          0\n",
              "4  Allah affetsin çok kötüydü hiç bir şey mi iyi ...          0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "### START YOUR CODE HERE ###\n",
        "\n",
        "df = pd.read_csv(HW_PATH + 'yorumsepeti.csv')\n",
        "\n",
        "### END YOUR CODE HERE ###\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCLl2dmhpRis",
        "outputId": "06efb4b7-5a30-4b58-eb33-0fdb29117322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample review: Çok geç geldi.\n",
            "Label: 0\n"
          ]
        }
      ],
      "source": [
        "reviews = df[\"review\"].tolist()\n",
        "labels = df[\"sentiment\"].tolist()\n",
        "\n",
        "print(\"Sample review: \" + reviews[42])\n",
        "print(\"Label: \" + str(labels[42]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6440Kaqhptw3"
      },
      "source": [
        "You should get:\n",
        "\n",
        "\n",
        "```\n",
        "Sample review: Çok geç geldi.\n",
        "Label: 0\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35h91uouqHGs"
      },
      "source": [
        "### **Pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6hwZClKqrZP"
      },
      "source": [
        "The first step when building a neural network model is getting your data into the proper form to feed into the network.\n",
        "\n",
        "Let's split every review into individual words, lemmatize those words and remove punctuation.\n",
        "\n",
        "If you're interested, consider reading this paper before getting started: https://arxiv.org/pdf/2003.07082\n",
        "\n",
        "This run takes approximately 55 minutes on an 8-thread Coffee Lake Refresh CPU with single-channel memory, and 12–13 minutes on a T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f1ebe8ba01734a6c87f7bce521eed8e4",
            "fa5fb7d151dd46ef9b464e8351ed7794",
            "835f378da5154cfdaefaf92d43c3af61",
            "0f40c7606060473abd9fa073a3fa2a45",
            "1b3ace4bdc2c4fccbb52211110401d4e",
            "c65ba0a1fc4c424ebc7bebc4c106eb7b",
            "d58e1ec18c86434c90075ca86a4f2f14",
            "fc28f52e4bf0457298672431ca9b9735",
            "76ed00ed86b64f4a9821d1285a12479a",
            "20dd9acd2f5149ae9b1226cf0aef66de",
            "179ba93596e14500afc794c16c491627",
            "200bfc0841004ba7adf8c3dd41536124",
            "3365dbef4d204d3cba29de1597bfcc52",
            "f6aa11edab8c43dba764aa1b90cfeb80",
            "1f0808bee0e4472fa076e89b31179d9f",
            "342754a004a24e808ad3151ff231ea62",
            "6e4fd7cec13e47c88508692d7dff943e",
            "bc9c6f8198d54c82b096a3f74e3fa635",
            "9bd16de49a07429195dc4e9b72ea8916",
            "c25381fcb77b4043bf21101e492336bb",
            "e2926c4374de4a2288e451f90a67b883",
            "5443cc754e564430acf06880d9228f8a"
          ]
        },
        "id": "_m4kJw2g-QiN",
        "outputId": "c9ff5139-a1b2-4612-bf4b-e3328d7ae710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stanza in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from stanza) (2.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from stanza) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (5.29.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from stanza) (3.4.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1ebe8ba01734a6c87f7bce521eed8e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: tr (Turkish) ...\n",
            "INFO:stanza:File exists: /root/stanza_resources/tr/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "200bfc0841004ba7adf8c3dd41536124",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: tr (Turkish):\n",
            "=============================\n",
            "| Processor | Package       |\n",
            "-----------------------------\n",
            "| tokenize  | imst          |\n",
            "| mwt       | imst          |\n",
            "| pos       | imst_charlm   |\n",
            "| lemma     | imst_nocharlm |\n",
            "=============================\n",
            "\n",
            "INFO:stanza:Using device: cuda\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Done loading processors!\n",
            "100%|██████████| 13751/13751 [12:58<00:00, 17.66it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install stanza\n",
        "import stanza\n",
        "import torch\n",
        "from string import punctuation\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download the Turkish Stanza model (should be run only once)\n",
        "stanza.download('tr')\n",
        "\n",
        "# Initialize the Stanza pipeline (includes lemmatizer)\n",
        "lemmatizer = stanza.Pipeline('tr', processors='tokenize,mwt,pos,lemma', use_gpu=True)\n",
        "\n",
        "reviews_processed = []\n",
        "\n",
        "# iterate through reviews and process\n",
        "for i in tqdm(range(len(reviews))):\n",
        "\n",
        "    doc = lemmatizer(reviews[i].lower())  # convert to lowercase and analyze\n",
        "    lemmas = [word.lemma for sent in doc.sentences for word in sent.words]  # extract lemmatized words\n",
        "\n",
        "    # Remove punctuation\n",
        "    filtered = [lemma for lemma in lemmas if lemma is not None and lemma not in punctuation]\n",
        "    reviews_processed.append(' '.join(filtered))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8tjCSAHI426"
      },
      "source": [
        "Save `reviews_processed` for future use.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nrnaT4j3CmSH"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(HW_PATH + '/reviews_processed.pkl', 'wb') as f:\n",
        "  pickle.dump(reviews_processed, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM03i2WOJCZe"
      },
      "source": [
        "Load `reviews_processed` if you are coming back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BHyGSUxdIJKX"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(HW_PATH + '/reviews_processed.pkl', 'rb') as f:\n",
        "    reviews_processed = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_U_z9VZ1cAjs",
        "outputId": "d9e76ae3-962c-4d2e-ed9d-0ea84f4ff2ea"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'her zaman komşu fır sipariş ver için eksik gönder tespit et tamamla için bilgi ver gönder 2 adet vişneli kurabi özür ol nere eski komşu fır hizmet anlayış'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reviews_processed[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc4zbyObuACd"
      },
      "source": [
        "## **3. Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdIYTsJYucRz"
      },
      "source": [
        "### **Create vocabulary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS9ZsZQYu3mF"
      },
      "source": [
        "Find unique words in the dataset and build a dictionary that maps words to integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CkgCmKbDufmY"
      },
      "outputs": [],
      "source": [
        "# feel free to use this import\n",
        "from collections import Counter\n",
        "\n",
        "### START YOUR CODE HERE ###\n",
        "\n",
        "# get unique list of words\n",
        "words = ' '.join(reviews_processed).split()\n",
        "word_counts = Counter(words)\n",
        "## Build a dictionary that maps words to integers\n",
        "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "vocab = {word: ii+1 for ii, word in enumerate(vocab)}\n",
        "\n",
        "### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEXwzqCNuCMh"
      },
      "source": [
        "### **Method 1: Encoding the words with integers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGB5kFLQuOsw"
      },
      "source": [
        "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network.\n",
        "\n",
        "> **Exercise:** Now you're going to encode the words with integers. Build a dictionary that maps words to integers. Later we're going to pad our input vectors with zeros, so make sure the integers **start at 1, not 0**.\n",
        "> Also, convert the words in reviews to integers and store the reviews in a new list called `reviews_ints`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cjKBkQuXsyrs"
      },
      "outputs": [],
      "source": [
        "### START YOUR CODE HERE ###\n",
        "\n",
        "vocab_to_int = vocab\n",
        "\n",
        "## use the dict to tokenize each review in reviews_split\n",
        "## store the tokenized reviews in reviews_ints\n",
        "reviews_ints = []\n",
        "for review in reviews_processed:\n",
        "    reviews_ints.append([vocab_to_int[word] for word in review.split() if word in vocab_to_int])\n",
        "\n",
        "### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYihgogCwlSZ"
      },
      "source": [
        "**Test your code**\n",
        "\n",
        "As a text that you've implemented the dictionary correctly, print out the number of unique words in your vocabulary and the contents of the first, tokenized review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eua6jhQEwQcD",
        "outputId": "f4568695-7640-448a-ffec-3f7afceb807d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique words:  9422\n",
            "\n",
            "Tokenized review: \n",
            " [[64, 85, 1694, 642, 9, 12, 27, 126, 50, 3158, 6, 1970, 27, 629, 12, 50, 73, 248, 3159, 1127, 615, 7, 379, 278, 1694, 642, 219, 1090]]\n"
          ]
        }
      ],
      "source": [
        "# stats about vocabulary\n",
        "print('Unique words: ', len((vocab_to_int)))  # should ~ 9900+\n",
        "print()\n",
        "\n",
        "# print tokens in first review\n",
        "print('Tokenized review: \\n', reviews_ints[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzk3WkIow9Ht"
      },
      "source": [
        "### **Method 2: Using fastText**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ix5VPjMxcxk"
      },
      "source": [
        "This might take several minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IW5DLS6-QiU",
        "outputId": "88006c83-ae15-4fa7-914d-8bf56191c2a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-24 13:46:46--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.254.124, 108.157.254.15, 108.157.254.102, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.254.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1261500728 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘/content/drive/MyDrive/YZV405E_HW4/cc.tr.300.vec.gz.2’\n",
            "\n",
            "cc.tr.300.vec.gz.2  100%[===================>]   1.17G  63.1MB/s    in 18s     \n",
            "\n",
            "2025-04-24 13:47:04 (66.4 MB/s) - ‘/content/drive/MyDrive/YZV405E_HW4/cc.tr.300.vec.gz.2’ saved [1261500728/1261500728]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tr.300.vec.gz -P \"{HW_PATH}\" #Specify the download path based on your desired destination directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BGcU0Tw0-QiU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "gz_file = os.path.join(HW_PATH, 'cc.tr.300.vec.gz')\n",
        "\n",
        "!gzip -d \"{gz_file}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTfKQJXjxASY",
        "outputId": "b7a1d0e9-eda2-4e31-8f6a-23401706ee32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.18.0\n",
            "  Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.18.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.18.0) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.18.0) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.18.0) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext==0.18.0) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.18.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.18.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.18.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0->torchtext==0.18.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.3.0->torchtext==0.18.0) (1.3.0)\n",
            "Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m150.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.11/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "  0%|          | 0/2000000 [00:00<?, ?it/s]WARNING:torchtext.vocab.vectors:Skipping token b'2000000' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|██████████| 2000000/2000000 [04:22<00:00, 7624.35it/s]\n"
          ]
        }
      ],
      "source": [
        "#The following versions must be used when working on Google Colab.\n",
        "#!pip install torch==2.3.0+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install torchtext==0.18.0\n",
        "HW_PATH = '/content/drive/MyDrive/YZV405E_HW4/'\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "# Load the fasttext word embedding for tweets\n",
        "FASTTEXT_EMB_FILE = HW_PATH + '/cc.tr.300.vec'\n",
        "emb = Vectors(name=FASTTEXT_EMB_FILE, cache= HW_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k75kLN0cyy4s"
      },
      "source": [
        "> **Exercise:** Now you're going to get word embeddings from fastText. Convert the words in reviews to integer indexes from fastText and store the reviews in a new list called `reviews_emb_ix`. You can use `emb.stoi[word]` to get the index of a word from fastText."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "P0IFAKi-yX1h"
      },
      "outputs": [],
      "source": [
        "### START YOUR CODE HERE ###\n",
        "\n",
        "reviews_emb_ix = []\n",
        "for review in reviews_processed:\n",
        "    reviews_emb_ix.append([emb.stoi[word] for word in review.split() if word in emb.stoi])\n",
        "\n",
        "### END YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZGPd0JTx4pZ"
      },
      "source": [
        "### **Padding sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A4EXpX2yH6H"
      },
      "source": [
        "To deal with both short and very long reviews, we'll pad or truncate all our reviews to a specific length. For reviews shorter than some `max_seq_length`, we'll pad with 0s. For reviews longer than `max_seq_length`, we can truncate them to the first `max_seq_length` words.\n",
        "\n",
        "> **Exercise:** Define a function that returns an array `padded_sequence` that contains the padded data, of a standard size, that we'll pass to the network.\n",
        "* The data should come from `reviews_ints` and `reviews_emb_ix`, since we want to feed integers to the network.\n",
        "* Each row should be `max_seq_length` elements long.\n",
        "* For reviews shorter than `max_seq_length` words, **left pad** with 0s. That is, if the review is `['en', 'iyi', 'kumru']`, `[117, 18, 128]` as integers, the row will look like `[0, 0, 0, ..., 0, 117, 18, 128]`.\n",
        "* For reviews longer than `max_seq_length`, use only the first `max_seq_length` words as the feature vector.\n",
        "\n",
        "As a small example, if the `max_seq_length=10` and an input review is:\n",
        "```\n",
        "[117, 18, 128]\n",
        "```\n",
        "The resultant, padded sequence should be:\n",
        "\n",
        "```\n",
        "[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\n",
        "```\n",
        "\n",
        "**Your final `padded_sequence` array should be a 2D array, with as many rows as there are reviews, and as many columns as the specified `max_seq_length`.**\n",
        "\n",
        "This isn't trivial and there are a bunch of ways to do this. But, if you're going to be building your own deep learning networks, you're going to have to get used to preparing your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jib53RbQxbKK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def pad_sequence(reviews_vecs, max_sequence_length):\n",
        "    ''' Return padded_sequence of review_vecs, where each review is padded with 0's\n",
        "        or truncated to the input max_sequence_length.\n",
        "    '''\n",
        "\n",
        "    ### START YOUR CODE HERE ###\n",
        "\n",
        "    # getting the correct rows x cols shape\n",
        "    padded_sequence = torch.zeros((len(reviews_vecs), max_sequence_length), dtype=torch.long)\n",
        "\n",
        "    # iterate through list of reviews\n",
        "    for i, review in enumerate(reviews_vecs):\n",
        "\n",
        "        # get the length of current review\n",
        "        sequence_length = len(review)\n",
        "\n",
        "        # iterate through words of current review\n",
        "        for j, token in enumerate(review):\n",
        "\n",
        "          # if token index exceeds max_sequence_length\n",
        "          if j >= max_sequence_length:\n",
        "            break\n",
        "\n",
        "          padded_sequence[i, max_sequence_length - min(sequence_length, max_sequence_length) + j] = token\n",
        "\n",
        "    ### END YOUR CODE HERE ###\n",
        "\n",
        "    return padded_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wQ5ykA2ph-5m"
      },
      "outputs": [],
      "source": [
        "padded_reviews_ints = pad_sequence(reviews_ints, max_sequence_length=30)\n",
        "\n",
        "padded_reviews_embs = pad_sequence(reviews_emb_ix, max_sequence_length=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn9wcs7RsLWD"
      },
      "source": [
        "See reviews before and after padding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3p7paF2ifob",
        "outputId": "990da0ca-24d9-41a3-9feb-a76983536c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[86, 324, 2, 362, 126, 1403, 2]\n",
            "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   86,\n",
            "         324,    2,  362,  126, 1403,    2])\n"
          ]
        }
      ],
      "source": [
        "print(reviews_ints[10])\n",
        "print(padded_reviews_ints[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEHnUL1VjAhY",
        "outputId": "028dde29-c3b8-453f-dbab-75d30996a8d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[136, 40322, 4047, 5112, 2712, 496561, 4047]\n",
            "tensor([     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,    136,  40322,   4047,   5112,\n",
            "          2712, 496561,   4047])\n"
          ]
        }
      ],
      "source": [
        "print(reviews_emb_ix[10])\n",
        "print(padded_reviews_embs[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hS7Z4UisTW9"
      },
      "source": [
        "**Test your code:** your `padded_sequence` should have as many rows as reviews and each `padded_sequence` row should contain `max_seq_length` values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JzNbdR-VjRIi"
      },
      "outputs": [],
      "source": [
        "assert len(padded_reviews_ints)==len(reviews_ints), \"Your padded_sequence should have as many rows as reviews.\"\n",
        "assert len(padded_reviews_ints[0])==30, \"Each padded_sequence row should contain max_seq_length values.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dCT7ecORsskU"
      },
      "outputs": [],
      "source": [
        "assert len(padded_reviews_embs)==len(reviews_emb_ix), \"Your padded_sequence should have as many rows as reviews.\"\n",
        "assert len(padded_reviews_embs[0])==30, \"Each padded_sequence row should contain max_seq_length values.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_lA5f8ttANB"
      },
      "source": [
        "## **4. Training, Validation, Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBJ1grQ-tEYG"
      },
      "source": [
        "With our data in nice shape, we'll split it into training, validation, and test sets.\n",
        "\n",
        "> **Exercise:** Create the training, validation, and test sets.\n",
        "* You'll need to create sets for the features and the labels, `train_x` and `train_y`, for example.\n",
        "* Define a split fraction, `split_frac` as the fraction of data to **keep** in the training set. Usually this is set to 0.8 or 0.9.\n",
        "* Whatever data is left will be split in half to create the validation and *testing* data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ML-kYEC9sx8-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def split(features, labels, split_frac=0.8):\n",
        "\n",
        "  ## split data into training, validation, and test data (features and labels, x and y)\n",
        "\n",
        "  ### START YOUR CODE HERE ###\n",
        "\n",
        "  split_idx = int(len(features)*split_frac)\n",
        "  train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "  train_y, remaining_y = labels[:split_idx], labels[split_idx:]\n",
        "\n",
        "  test_idx = int(len(remaining_x)*0.5)\n",
        "  val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "  val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "  ### END YOUR CODE HERE ###\n",
        "\n",
        "  ## print out the shapes of your resultant feature data\n",
        "  print(\"\\t\\t\\tFeature Shapes:\")\n",
        "  print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
        "        \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "        \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
        "  print(\"\\n\")\n",
        "\n",
        "  return [np.array(train_x), np.array(train_y), np.array(test_x), np.array(test_y), np.array(val_x), np.array(val_y)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPnfpsgQuAeG",
        "outputId": "e5ce91a9-bc53-4d34-e158-5a233c3d2c25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\ttorch.Size([11000, 30]) \n",
            "Validation set: \ttorch.Size([1375, 30]) \n",
            "Test set: \t\ttorch.Size([1376, 30])\n",
            "\n",
            "\n",
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\ttorch.Size([11000, 30]) \n",
            "Validation set: \ttorch.Size([1375, 30]) \n",
            "Test set: \t\ttorch.Size([1376, 30])\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "reviews_ints_split = split(padded_reviews_ints, labels)\n",
        "\n",
        "reviews_embs_split = split(padded_reviews_embs, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFESLIYXuq5t"
      },
      "source": [
        "**Check your work**\n",
        "\n",
        "With train, validation, and test fractions equal to 0.8, 0.1, 0.1, respectively, the final, feature data shapes should look like:\n",
        "```\n",
        "                    Feature Shapes:\n",
        "Train set: \t\t (11000, 30)\n",
        "Validation set: \t(1375, 30)\n",
        "Test set: \t\t  (1375, 30)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLQLnrXiu4op"
      },
      "source": [
        "### **DataLoaders and Batching**\n",
        "\n",
        "After creating training, test, and validation data, we can create DataLoaders for this data by following two steps:\n",
        "1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
        "2. Create DataLoaders and batch our training, validation, and test Tensor datasets.\n",
        "\n",
        "```\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "```\n",
        "\n",
        "This is an alternative to creating a generator function for batching our data into full batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xX-wELRIuaTQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def batching(split_list, batch_size=32):\n",
        "\n",
        "  ### START YOUR CODE HERE ###\n",
        "\n",
        "  # create Tensor datasets\n",
        "  train_data = TensorDataset(torch.from_numpy(split_list[0]), torch.from_numpy(split_list[1]))\n",
        "  valid_data = TensorDataset(torch.from_numpy(split_list[4]), torch.from_numpy(split_list[5]))\n",
        "  test_data = TensorDataset(torch.from_numpy(split_list[2]), torch.from_numpy(split_list[3]))\n",
        "\n",
        "  # make sure the SHUFFLE your training data\n",
        "  train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "  valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "  test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "  ### END YOUR CODE HERE ###\n",
        "\n",
        "  return [train_data, valid_data, test_data, train_loader, valid_loader, test_loader]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xcUBYOfvvs_W"
      },
      "outputs": [],
      "source": [
        "reviews_ints_batch = batching(reviews_ints_split)\n",
        "\n",
        "reviews_embs_batch = batching(reviews_embs_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2xh_zVYx3NX"
      },
      "source": [
        "### **Sentiment Network**\n",
        "\n",
        "Below is where you'll define the network.\n",
        "\n",
        "The layers are as follows:\n",
        "1. An [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) that converts our word tokens (integers) into embeddings of a specific size.\n",
        "2. An [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) defined by a hidden_state size and number of layers.\n",
        "3. A fully-connected output layer that maps the LSTM layer outputs to a desired output_size.\n",
        "4. A sigmoid activation layer which turns all outputs into a value 0-1; return **only the last sigmoid output** as the output of this network.\n",
        "\n",
        "#### **The Embedding Layer**\n",
        "\n",
        "We need to add an [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding). It is massively inefficient to one-hot encode that many classes. So, instead of one-hot encoding, we can have an embedding layer and use that layer as a lookup table.\n",
        "\n",
        "#### **The LSTM Layer(s)**\n",
        "\n",
        "We'll create an [LSTM](https://pytorch.org/docs/stable/nn.html#lstm) to use in our recurrent network, which takes in an input_size, a hidden_dim, a number of layers, a dropout probability (for dropout between multiple layers), and a batch_first parameter.\n",
        "\n",
        "Most of the time, you're network will have better performance with more layers; between 2-3. Adding more layers allows the network to learn really complex relationships.\n",
        "\n",
        "> **Exercise:** Complete the `__init__` and `forward` functions for the SentimentRNN model class.\n",
        "\n",
        "Note: `init_hidden` should initialize the hidden and cell state of an lstm layer to all zeros, and move those state to GPU, if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eOgGropv3AJ",
        "outputId": "771540fc-e2e8-4423-e000-8378038a5a20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ]
        }
      ],
      "source": [
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "INgkrY3xyU2E"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5, pretrained=False):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        ### START YOUR CODE HERE ###\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # embedding and LSTM layers\n",
        "        if not pretrained:\n",
        "          self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        else:\n",
        "          self.embedding = nn.Embedding.from_pretrained(emb.vectors)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "        ### END YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "\n",
        "        ### START YOUR CODE HERE ###\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        lstm_out = lstm_out[:, -1, :] # getting the last time step output\n",
        "\n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "\n",
        "        ### END YOUR CODE HERE ###\n",
        "\n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "\n",
        "        return hidden\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ2QT8Dq22g2"
      },
      "source": [
        "### **Instantiate the network**\n",
        "\n",
        "Here, we'll instantiate the network. First up, defining the hyperparameters.\n",
        "\n",
        "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
        "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
        "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
        "* `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
        "* `n_layers`: Number of LSTM layers in the network. Typically between 1-3\n",
        "\n",
        "> **Exercise:** Define the model  hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "delqSMBP2eTF",
        "outputId": "fa69ae77-5a3c-404a-c88c-be63556bae19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(9423, 300)\n",
            "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n",
            "SentimentRNN(\n",
            "  (embedding): Embedding(2000000, 300)\n",
            "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "### START YOUR CODE HERE ###\n",
        "\n",
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int) + 1\n",
        "output_size = 1\n",
        "embedding_dim = 300\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "### END YOUR CODE HERE ###\n",
        "\n",
        "net_int = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "net_emb = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, pretrained=True)\n",
        "\n",
        "print(net_int)\n",
        "\n",
        "print(net_emb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoAPduLt3mJ4"
      },
      "source": [
        "### **Training & Testing**\n",
        "Below is the typical training code. You can add code to save a model by name.\n",
        "\n",
        ">We'll also be using a kind of cross entropy loss, which is designed to work with a single Sigmoid output. [BCELoss](https://pytorch.org/docs/stable/nn.html#bceloss), or **Binary Cross Entropy Loss**, applies cross entropy loss to a single value between 0 and 1.\n",
        "\n",
        "We also have some data and training hyparameters:\n",
        "\n",
        "* `lr`: Learning rate for our optimizer.\n",
        "* `epochs`: Number of times to iterate through the training dataset.\n",
        "* `clip`: The maximum gradient value to clip at (to prevent exploding gradients).\n",
        "\n",
        "* **Test data performance:** We'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tY7eZyqz3Jcn"
      },
      "outputs": [],
      "source": [
        "def train_test(net, train_loader, valid_loader, test_loader, batch_size=32):\n",
        "  # loss and optimization functions\n",
        "  lr=0.001\n",
        "\n",
        "  criterion = nn.BCELoss()\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "  # training params\n",
        "\n",
        "  epochs = 4\n",
        "\n",
        "  counter = 0\n",
        "  print_every = 100\n",
        "  clip=5 # gradient clipping\n",
        "\n",
        "  # move model to GPU, if available\n",
        "  if(train_on_gpu):\n",
        "      net.cuda()\n",
        "\n",
        "  net.train()\n",
        "  # train for some number of epochs\n",
        "  for e in range(epochs):\n",
        "      # initialize hidden state\n",
        "      h = net.init_hidden(batch_size)\n",
        "\n",
        "      # batch loop\n",
        "      for inputs, labels in train_loader:\n",
        "          counter += 1\n",
        "\n",
        "          if(train_on_gpu):\n",
        "              inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "          # Creating new variables for the hidden state, otherwise\n",
        "          # we'd backprop through the entire training history\n",
        "          h = tuple([each.data for each in h])\n",
        "\n",
        "          # zero accumulated gradients\n",
        "          net.zero_grad()\n",
        "          # get the output from the model\n",
        "          output, h = net(inputs, h)\n",
        "\n",
        "          # calculate the loss and perform backprop\n",
        "          loss = criterion(output.squeeze(), labels.float())\n",
        "          loss.backward()\n",
        "          # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "          nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "          optimizer.step()\n",
        "\n",
        "          # loss stats\n",
        "          if counter % print_every == 0:\n",
        "              # Get validation loss\n",
        "              val_h = net.init_hidden(batch_size)\n",
        "              val_losses = []\n",
        "              net.eval()\n",
        "              for inputs, labels in valid_loader:\n",
        "\n",
        "                  # Creating new variables for the hidden state, otherwise\n",
        "                  # we'd backprop through the entire training history\n",
        "                  val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                  if(train_on_gpu):\n",
        "                      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                  output, val_h = net(inputs, val_h)\n",
        "                  val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                  val_losses.append(val_loss.item())\n",
        "\n",
        "              net.train()\n",
        "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "\n",
        "  \"\"\"\n",
        "  STARTING TO TEST\n",
        "  \"\"\"\n",
        "\n",
        "  # Get test data loss and accuracy\n",
        "\n",
        "  test_losses = [] # track loss\n",
        "  num_correct = 0\n",
        "\n",
        "  # init hidden state\n",
        "  h = net.init_hidden(batch_size)\n",
        "\n",
        "  net.eval()\n",
        "  # iterate over test data\n",
        "  for inputs, labels in test_loader:\n",
        "\n",
        "      # Creating new variables for the hidden state, otherwise\n",
        "      # we'd backprop through the entire training history\n",
        "      h = tuple([each.data for each in h])\n",
        "\n",
        "      if(train_on_gpu):\n",
        "          inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "      # get predicted outputs\n",
        "      output, h = net(inputs, h)\n",
        "\n",
        "      # calculate loss\n",
        "      test_loss = criterion(output.squeeze(), labels.float())\n",
        "      test_losses.append(test_loss.item())\n",
        "\n",
        "      # convert output probabilities to predicted class (0 or 1)\n",
        "      pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "\n",
        "      # compare predictions to true label\n",
        "      correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "      correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "      num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "  # -- stats! -- ##\n",
        "  # avg test loss\n",
        "  print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "  # accuracy over all test data\n",
        "  test_acc = num_correct/len(test_loader.dataset)\n",
        "  print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ5Ai-KIzyDg"
      },
      "source": [
        "A minimum test accuracy of 84% is required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw6bPXmD3453",
        "outputId": "68ac9e11-35b2-4d6d-a376-db8f500b8ac3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/4... Step: 100... Loss: 0.355172... Val Loss: 0.429872\n",
            "Epoch: 1/4... Step: 200... Loss: 0.246268... Val Loss: 0.321698\n",
            "Epoch: 1/4... Step: 300... Loss: 0.249701... Val Loss: 0.296915\n",
            "Epoch: 2/4... Step: 400... Loss: 0.258323... Val Loss: 0.287476\n",
            "Epoch: 2/4... Step: 500... Loss: 0.280142... Val Loss: 0.266661\n",
            "Epoch: 2/4... Step: 600... Loss: 0.116212... Val Loss: 0.266135\n",
            "Epoch: 3/4... Step: 700... Loss: 0.112436... Val Loss: 0.279043\n",
            "Epoch: 3/4... Step: 800... Loss: 0.364446... Val Loss: 0.331357\n",
            "Epoch: 3/4... Step: 900... Loss: 0.329708... Val Loss: 0.281977\n",
            "Epoch: 3/4... Step: 1000... Loss: 0.159596... Val Loss: 0.291328\n",
            "Epoch: 4/4... Step: 1100... Loss: 0.008357... Val Loss: 0.345549\n",
            "Epoch: 4/4... Step: 1200... Loss: 0.245313... Val Loss: 0.400551\n",
            "Epoch: 4/4... Step: 1300... Loss: 0.051134... Val Loss: 0.370692\n",
            "Test loss: 0.401\n",
            "Test accuracy: 0.887\n"
          ]
        }
      ],
      "source": [
        "train_test(net_int, reviews_ints_batch[3], reviews_ints_batch[4], reviews_ints_batch[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of8ZmzuW5lIC",
        "outputId": "e034ad3b-6aeb-4e6b-c0e2-c5a8e43393b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/4... Step: 100... Loss: 0.537815... Val Loss: 0.522613\n",
            "Epoch: 1/4... Step: 200... Loss: 0.345190... Val Loss: 0.370334\n",
            "Epoch: 1/4... Step: 300... Loss: 0.343093... Val Loss: 0.338043\n",
            "Epoch: 2/4... Step: 400... Loss: 0.259589... Val Loss: 0.316154\n",
            "Epoch: 2/4... Step: 500... Loss: 0.477005... Val Loss: 0.325998\n",
            "Epoch: 2/4... Step: 600... Loss: 0.277201... Val Loss: 0.327965\n",
            "Epoch: 3/4... Step: 700... Loss: 0.204179... Val Loss: 0.286772\n",
            "Epoch: 3/4... Step: 800... Loss: 0.203407... Val Loss: 0.289831\n",
            "Epoch: 3/4... Step: 900... Loss: 0.330558... Val Loss: 0.287656\n",
            "Epoch: 3/4... Step: 1000... Loss: 0.325915... Val Loss: 0.285366\n",
            "Epoch: 4/4... Step: 1100... Loss: 0.220127... Val Loss: 0.325600\n",
            "Epoch: 4/4... Step: 1200... Loss: 0.183294... Val Loss: 0.294614\n",
            "Epoch: 4/4... Step: 1300... Loss: 0.185305... Val Loss: 0.323806\n",
            "Test loss: 0.275\n",
            "Test accuracy: 0.881\n"
          ]
        }
      ],
      "source": [
        "train_test(net_emb, reviews_embs_batch[3], reviews_embs_batch[4], reviews_embs_batch[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXzV-EIi5raw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f40c7606060473abd9fa073a3fa2a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20dd9acd2f5149ae9b1226cf0aef66de",
            "placeholder": "​",
            "style": "IPY_MODEL_179ba93596e14500afc794c16c491627",
            "value": " 426k/? [00:00&lt;00:00, 30.9MB/s]"
          }
        },
        "179ba93596e14500afc794c16c491627": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b3ace4bdc2c4fccbb52211110401d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f0808bee0e4472fa076e89b31179d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2926c4374de4a2288e451f90a67b883",
            "placeholder": "​",
            "style": "IPY_MODEL_5443cc754e564430acf06880d9228f8a",
            "value": " 426k/? [00:00&lt;00:00, 32.2MB/s]"
          }
        },
        "200bfc0841004ba7adf8c3dd41536124": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3365dbef4d204d3cba29de1597bfcc52",
              "IPY_MODEL_f6aa11edab8c43dba764aa1b90cfeb80",
              "IPY_MODEL_1f0808bee0e4472fa076e89b31179d9f"
            ],
            "layout": "IPY_MODEL_342754a004a24e808ad3151ff231ea62"
          }
        },
        "20dd9acd2f5149ae9b1226cf0aef66de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3365dbef4d204d3cba29de1597bfcc52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e4fd7cec13e47c88508692d7dff943e",
            "placeholder": "​",
            "style": "IPY_MODEL_bc9c6f8198d54c82b096a3f74e3fa635",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "342754a004a24e808ad3151ff231ea62": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5443cc754e564430acf06880d9228f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e4fd7cec13e47c88508692d7dff943e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76ed00ed86b64f4a9821d1285a12479a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "835f378da5154cfdaefaf92d43c3af61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc28f52e4bf0457298672431ca9b9735",
            "max": 52741,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76ed00ed86b64f4a9821d1285a12479a",
            "value": 52741
          }
        },
        "9bd16de49a07429195dc4e9b72ea8916": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc9c6f8198d54c82b096a3f74e3fa635": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c25381fcb77b4043bf21101e492336bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c65ba0a1fc4c424ebc7bebc4c106eb7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d58e1ec18c86434c90075ca86a4f2f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2926c4374de4a2288e451f90a67b883": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1ebe8ba01734a6c87f7bce521eed8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa5fb7d151dd46ef9b464e8351ed7794",
              "IPY_MODEL_835f378da5154cfdaefaf92d43c3af61",
              "IPY_MODEL_0f40c7606060473abd9fa073a3fa2a45"
            ],
            "layout": "IPY_MODEL_1b3ace4bdc2c4fccbb52211110401d4e"
          }
        },
        "f6aa11edab8c43dba764aa1b90cfeb80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bd16de49a07429195dc4e9b72ea8916",
            "max": 52741,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c25381fcb77b4043bf21101e492336bb",
            "value": 52741
          }
        },
        "fa5fb7d151dd46ef9b464e8351ed7794": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c65ba0a1fc4c424ebc7bebc4c106eb7b",
            "placeholder": "​",
            "style": "IPY_MODEL_d58e1ec18c86434c90075ca86a4f2f14",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "fc28f52e4bf0457298672431ca9b9735": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
